# üöÄ Configurar Ollama Local com T√∫nel

Este guia explica como conectar o Ollama rodando no seu PC local ao CRM no servidor usando um t√∫nel.

## üìã Pr√©-requisitos

1. **Ollama instalado no seu PC**
   - Download: https://ollama.ai
   - Instale e baixe um modelo: `ollama pull llama3` (ou outro modelo)

2. **Ferramenta de t√∫nel** (escolha uma):
   - **ngrok** (recomendado - mais f√°cil)
   - **Cloudflare Tunnel** (gratuito, sem limite)
   - **localtunnel** (gratuito, simples)

## üîß Op√ß√£o 1: Usando ngrok (Recomendado)

### Passo 1: Instalar ngrok
- Windows: Baixe de https://ngrok.com/download
- Ou via chocolatey: `choco install ngrok`
- Ou via npm: `npm install -g ngrok`

### Passo 2: Criar conta e obter token
1. Crie conta gratuita em https://ngrok.com
2. Copie seu authtoken da dashboard
3. Configure: `ngrok config add-authtoken SEU_TOKEN`

### Passo 3: Iniciar t√∫nel
```bash
ngrok http 11434
```

Isso vai gerar uma URL como: `https://abc123.ngrok-free.app`

### Passo 4: Configurar no CRM
1. Acesse o CRM como admin
2. V√° em "Configura√ß√£o IA"
3. Crie nova configura√ß√£o:
   - **Nome**: "Ollama Local (ngrok)"
   - **Provider**: `ollama`
   - **Model Name**: `llama3` (ou o modelo que voc√™ baixou)
   - **Base URL**: `https://abc123.ngrok-free.app` (URL do ngrok)
   - **API Key**: deixe vazio (Ollama n√£o precisa)
   - Marque como **Ativo** e **Padr√£o**

### Passo 5: Testar
Use o bot√£o "Testar Conex√£o" na configura√ß√£o do CRM.

**‚ö†Ô∏è Importante**: A URL do ngrok muda a cada rein√≠cio (no plano gratuito). Voc√™ precisar√° atualizar a configura√ß√£o no CRM quando reiniciar o ngrok.

**üí° Solu√ß√£o**: Use ngrok com dom√≠nio fixo (pago) ou Cloudflare Tunnel (gratuito com dom√≠nio fixo).

---

## üîß Op√ß√£o 2: Usando Cloudflare Tunnel (Gratuito, Dom√≠nio Fixo)

### Passo 1: Instalar cloudflared
- Windows: Baixe de https://github.com/cloudflare/cloudflared/releases
- Ou via chocolatey: `choco install cloudflared`

### Passo 2: Criar t√∫nel
```bash
cloudflared tunnel --url http://localhost:11434
```

Isso vai gerar uma URL como: `https://abc123.trycloudflare.com`

### Passo 3: Configurar no CRM
Mesmo processo da Op√ß√£o 1, mas use a URL do Cloudflare.

**‚úÖ Vantagem**: A URL permanece a mesma enquanto o t√∫nel estiver ativo.

---

## üîß Op√ß√£o 3: Usando localtunnel (Mais Simples)

### Passo 1: Instalar
```bash
npm install -g localtunnel
```

### Passo 2: Iniciar t√∫nel
```bash
lt --port 11434
```

Isso vai gerar uma URL como: `https://abc123.loca.lt`

### Passo 3: Configurar no CRM
Mesmo processo, use a URL do localtunnel.

**‚ö†Ô∏è Limita√ß√£o**: URLs mudam a cada rein√≠cio.

---

## üîß Op√ß√£o 4: SSH Tunnel (Mais Seguro, Requer Acesso SSH)

Se voc√™ tem acesso SSH ao servidor:

### No seu PC (Windows):
```bash
ssh -R 11434:localhost:11434 usuario@servidor
```

### No servidor, configure no CRM:
- **Base URL**: `http://localhost:11434`

**‚úÖ Vantagem**: Mais seguro, n√£o exp√µe para internet.

---

## üìù Configura√ß√£o Recomendada no CRM

```
Nome: Ollama Local
Provider: ollama
Model Name: llama3 (ou llama3:70b, mistral, etc.)
Base URL: [URL do seu t√∫nel]
API Key: (deixe vazio)
Ativo: ‚úÖ
Padr√£o: ‚úÖ
```

## üß™ Modelos Recomendados para Ollama

- **llama3** - Bom equil√≠brio velocidade/qualidade
- **llama3:70b** - Mais poderoso, mais lento
- **mistral** - R√°pido e eficiente
- **mixtral** - Mais poderoso que mistral
- **neural-chat** - Otimizado para conversas

## ‚ö†Ô∏è Considera√ß√µes

1. **Performance**: Ollama local pode ser mais lento que APIs cloud
2. **Recursos**: Modelos grandes precisam de RAM (llama3:70b precisa ~40GB)
3. **Lat√™ncia**: Depende da sua conex√£o com o servidor
4. **Disponibilidade**: Seu PC precisa estar ligado e com t√∫nel ativo

## üîÑ Manter T√∫nel Ativo

Para manter o t√∫nel rodando em background no Windows:

### Com ngrok:
Crie um arquivo `start-ngrok.bat`:
```batch
@echo off
cd /d "C:\caminho\para\ngrok"
ngrok http 11434
pause
```

### Com Cloudflare:
```batch
@echo off
cloudflared tunnel --url http://localhost:11434
pause
```

Ou use um servi√ßo Windows para iniciar automaticamente.

## üêõ Troubleshooting

**Erro de conex√£o:**
- Verifique se o Ollama est√° rodando: `ollama list`
- Teste localmente: `curl http://localhost:11434/api/tags`
- Verifique se o t√∫nel est√° ativo
- Verifique firewall do Windows

**Timeout:**
- Aumente o timeout no c√≥digo (j√° est√° em 120s)
- Verifique sua conex√£o de internet

**Modelo n√£o encontrado:**
- Baixe o modelo: `ollama pull llama3`
- Verifique o nome do modelo na configura√ß√£o

