# üîç Diagnosticar Problemas com Ollama

## ‚ùå Modelo "ola" n√£o conecta

### Passo 1: Verificar se o modelo existe no Ollama

No PowerShell, execute:

```powershell
ollama list
```

**Verifique:**
- O modelo "ola" aparece na lista?
- O nome est√° escrito exatamente como aparece? (pode ser "ola", "Ola", "OLA", etc.)

**Se o modelo N√ÉO aparecer:**
```powershell
# Baixe o modelo
ollama pull ola

# Ou verifique se o nome est√° correto
ollama list
```

### Passo 2: Testar o modelo localmente

```powershell
# Teste se o modelo funciona localmente
ollama run ola
```

Se funcionar localmente, o problema √© na conex√£o com o servidor.

### Passo 3: Verificar se o t√∫nel est√° ativo

**Cloudflare:**
- Verifique se o PowerShell com `cloudflared tunnel` ainda est√° aberto
- A URL pode ter mudado se voc√™ reiniciou o t√∫nel

**Teste a URL do t√∫nel:**
```powershell
# Substitua pela sua URL atual
curl https://SUA-URL.trycloudflare.com/api/tags
```

Se retornar JSON com os modelos, o t√∫nel est√° funcionando.

### Passo 4: Verificar configura√ß√£o no CRM

No CRM, verifique:

1. **Base URL:**
   - Deve ser a URL completa do t√∫nel (ex: `https://become-particles-affair-listed.trycloudflare.com`)
   - **SEM barra no final!**
   - **SEM `http://` ou `https://` duplicado**

2. **Model Name:**
   - Deve ser exatamente o nome que aparece em `ollama list`
   - **Case-sensitive!** (ola ‚â† Ola ‚â† OLA)
   - Sem espa√ßos extras

3. **Provider:**
   - Deve ser `ollama`

4. **Status:**
   - Deve estar marcado como **Ativo** e **Padr√£o**

### Passo 5: Testar conex√£o no CRM

1. Clique em "Testar Conex√£o" na configura√ß√£o
2. Veja a mensagem de erro (agora mais detalhada)

**Erros comuns e solu√ß√µes:**

- **"Modelo 'ola' n√£o encontrado"**
  - O modelo n√£o est√° instalado ou o nome est√° errado
  - Solu√ß√£o: `ollama pull ola` ou corrija o nome

- **"N√£o foi poss√≠vel conectar ao Ollama"**
  - T√∫nel n√£o est√° ativo ou URL errada
  - Solu√ß√£o: Verifique se o t√∫nel est√° rodando e use a URL correta

- **"Timeout"**
  - Conex√£o lenta ou modelo muito pesado
  - Solu√ß√£o: Aguarde ou use um modelo menor

### Passo 6: Verificar logs do backend

```bash
docker logs crm-backend --tail 50 | grep -i ollama
```

Isso mostra erros detalhados do backend.

## ‚úÖ Checklist de Verifica√ß√£o

- [ ] Modelo "ola" existe em `ollama list`
- [ ] Modelo funciona localmente: `ollama run ola`
- [ ] T√∫nel Cloudflare/ngrok est√° rodando
- [ ] URL do t√∫nel est√° correta no CRM (sem barra no final)
- [ ] Nome do modelo no CRM est√° exatamente igual ao `ollama list`
- [ ] Configura√ß√£o est√° marcada como Ativo e Padr√£o
- [ ] Teste de conex√£o no CRM mostra erro espec√≠fico

## üÜò Ainda n√£o funciona?

1. **Tente outro modelo:**
   ```powershell
   ollama pull llama3.1
   ```
   Use `llama3.1` no CRM

2. **Verifique se o Ollama est√° rodando:**
   ```powershell
   curl http://localhost:11434/api/tags
   ```

3. **Reinicie o t√∫nel:**
   - Feche o t√∫nel atual
   - Rode novamente: `cloudflared tunnel --url http://localhost:11434`
   - Use a nova URL no CRM

4. **Teste direto pelo t√∫nel:**
   ```powershell
   curl https://SUA-URL/api/generate -d '{"model":"ola","prompt":"teste","stream":false}'
   ```


